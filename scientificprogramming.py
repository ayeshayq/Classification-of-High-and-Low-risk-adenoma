# -*- coding: utf-8 -*-
"""Scientific_Programminggggg

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ay2mF9oI2H9b3kEU6vqsWdE7ddmAtuiD
"""

# Import libraries
import pandas as pd
from google.colab import files

# Upload the file
uploaded = files.upload()
filename = next(iter(uploaded))

# Read the Excel file into a DataFrame
df = pd.read_excel(filename, engine='openpyxl')  # Use 'xlrd' if it's .xls

import pandas as pd

# Read Excel and treat "n.a." as missing
df = pd.read_excel("SP.CRC.HR&LR.xlsx", engine='openpyxl', na_values=["n.a.", "N.A.", "NA"])

# Define metabolite columns (from '2-Pentene, (Z)' to the end)
metabolite_start_col = df.columns.get_loc('2-Pentene, (Z)')
metabolite_cols = df.columns[metabolite_start_col:]

# Filter out blank samples (0 = ambient, 1 = equipment blanks)
df_samples = df[~df['Study subgroup'].isin([0, 1])]

# Calculate missingness per subgroup
missing_summary = {}
for subgroup in [2, 3]:
    subgroup_df = df_samples[df_samples['Study subgroup'] == subgroup]
    missing_count = subgroup_df[metabolite_cols].isna().sum()
    missing_percentage = subgroup_df[metabolite_cols].isna().mean() * 100
    missing_summary[subgroup] = pd.DataFrame({
        "Missing Count": missing_count,
        "Missing Percentage": missing_percentage
    })

# Example: show missingness for subgroup 2
pd.set_option('display.max_rows', None)
print("Missingness for Study Subgroup 2:")
print(missing_summary[2])

# Example: show missingness for subgroup 3
print("\nMissingness for Study Subgroup 3:")
print(missing_summary[3])

import pandas as pd
from google.colab import files

# Read Excel and treat "n.a." as missing
df = pd.read_excel("SP.CRC.HR&LR.xlsx", engine='openpyxl', na_values=["n.a.", "N.A.", "NA"])

# Define metabolite columns (from '2-Pentene, (Z)' to the end)
metabolite_start_col = df.columns.get_loc('2-Pentene, (Z)')
metabolite_cols = df.columns[metabolite_start_col:]

# Copy the full dataframe for cleaning
df_cleaned = df.copy()

# Threshold for missingness
threshold = 30

# Loop only over real samples (subgroups 2 & 3) for missingness
for subgroup in [2, 3]:
    # Select rows of this subgroup
    subgroup_mask = df['Study subgroup'] == subgroup

    # Calculate missing percentage for metabolites in this subgroup
    missing_percentage = df.loc[subgroup_mask, metabolite_cols].isna().mean() * 100

    # Identify columns exceeding threshold
    cols_to_remove = missing_percentage[missing_percentage > threshold].index

    # Set these columns to NA **only for this subgroup**
    df_cleaned.loc[subgroup_mask, cols_to_remove] = pd.NA

# Save the cleaned dataframe to Excel
file_name = "SP_CRC_HR_LR_cleaned.xlsx"
df_cleaned.to_excel(file_name, index=False)

# Trigger download to your computer (Colab)
files.download(file_name)

# Define real subgroups
subgroups = [2, 3]

for sg in subgroups:
    print(f"\nSubgroup {sg} missingness:")
    sg_mask = df_cleaned['Study subgroup'] == sg
    missing_counts = df_cleaned.loc[sg_mask, metabolite_cols].isna().sum()
    missing_percent = df_cleaned.loc[sg_mask, metabolite_cols].isna().mean() * 100
    missing_summary = pd.DataFrame({
        "Missing Count": missing_counts,
        "Missing %": missing_percent
    })
    print(missing_summary)

# Define real subgroups
subgroups = [2, 3]

# Get columns with at least one non-NA value per subgroup
cols_non_na = {}
for sg in subgroups:
    sg_mask = df_cleaned['Study subgroup'] == sg
    cols_non_na[sg] = df_cleaned.loc[sg_mask, metabolite_cols].columns[
        df_cleaned.loc[sg_mask, metabolite_cols].notna().any()
    ].tolist()

# Shared columns (present in both)
shared_cols = list(set(cols_non_na[2]).intersection(set(cols_non_na[3])))

# Unique columns
unique_2 = list(set(cols_non_na[2]) - set(cols_non_na[3]))
unique_3 = list(set(cols_non_na[3]) - set(cols_non_na[2]))

# Print counts and optionally the column names
print(f"Shared columns (present in both subgroups): {len(shared_cols)}")
print(shared_cols)

print(f"\nUnique columns in subgroup 2: {len(unique_2)}")
print(unique_2)

print(f"\nUnique columns in subgroup 3: {len(unique_3)}")
print(unique_3)

"""FOR EACH COLUMN:
* EITHER ITS MISSING IN BOTH SUBGROUPS BECAUSE THERE'S TOO MUCH MISSINGNESS --> REMOVE ENTIRELY
* ITS DETECTED IN BOTH SUBGROUPS BUT THERE'S FEW MISSING VALUES --> IMPUTE THE MISSING VALUES
* MISSINGNESS HIGH IN ONE GROUP BUT NOT THE OTHER --> KEEP AS MISSING
"""

from google.colab import files
import pandas as pd

# Upload the file
uploaded = files.upload()  # This will open a file browser to select SP_CRC_HR_LR_cleaned.xlsx

# Read the uploaded Excel
df_cleaned = pd.read_excel("SP_CRC_HR_LR_cleaned.xlsx", engine='openpyxl')

import pandas as pd
from google.colab import files

# Read the cleaned Excel
df_cleaned = pd.read_excel("SP_CRC_HR_LR_cleaned.xlsx", engine='openpyxl')

# Define metabolite columns
metabolite_start_col = df_cleaned.columns.get_loc('2-Pentene, (Z)')
metabolite_cols = df_cleaned.columns[metabolite_start_col:]

# Mask for real samples (high-risk and low-risk)
real_samples_mask = df_cleaned['Study subgroup'].isin([2, 3])

# Impute missing values **only for real samples**
for col in metabolite_cols:
    median_val = df_cleaned.loc[real_samples_mask, col].median(skipna=True)
    df_cleaned.loc[real_samples_mask, col] = df_cleaned.loc[real_samples_mask, col].fillna(median_val)

# Save the imputed dataframe
file_name = "SP_CRC_HR_LR_imputed_real_samples.xlsx"
df_cleaned.to_excel(file_name, index=False)

# Download
files.download(file_name)

"""After imputation : compare blanks and study subgroups

"""

import pandas as pd
from scipy.stats import mannwhitneyu
from google.colab import files

# Load the cleaned dataset
df = pd.read_excel("SP_CRC_HR_LR_imputed_real_samples.xlsx", engine='openpyxl')

# Define metabolite columns (from '2-Pentene, (Z)' onward)
metabolite_start_col = df.columns.get_loc('2-Pentene, (Z)')
metabolite_cols = df.columns[metabolite_start_col:]

# Define groups
real_mask = df['Study subgroup'].isin([2, 3])   # biological samples
blank_mask = df['Study subgroup'].isin([0, 1])  # blanks

# Store significant VOCs
significant_vocs = []

# Loop over metabolites and run Mann-Whitney test
for col in metabolite_cols:
    real_vals = df.loc[real_mask, col].dropna()
    blank_vals = df.loc[blank_mask, col].dropna()

    if len(real_vals) > 0 and len(blank_vals) > 0:  # only test if both groups have data
        stat, pval = mannwhitneyu(real_vals, blank_vals, alternative='two-sided')
        if pval < 0.05:  # keep only significant VOCs
            significant_vocs.append(col)

# Create new dataframe with metadata + only significant VOCs
df_filtered = pd.concat([df.iloc[:, :metabolite_start_col], df[significant_vocs]], axis=1)

# Save filtered dataset
file_name = "SP_CRC_HR_LR_filtered_last.xlsx"
df_filtered.to_excel(file_name, index=False)


# Download
files.download(file_name)

print(f"✅ Kept {len(significant_vocs)} VOCs out of {len(metabolite_cols)}")
print(f"📂 Saved as: {file_name}")

from google.colab import files
files.download("SP_CRC_HR_LR_filtered_last.xlsx")

"""# PRE-PROCESSING COMPLETE + FINAL FILE"""

# --- ML Pipeline ---
import pandas as pd
import numpy as np
from google.colab import files
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# Upload the filtered dataset
uploaded = files.upload()
df = pd.read_excel("SP_CRC_HR_LR_filtered_last.xlsx", engine="openpyxl")

# Define target (Study subgroup: 2 = HR, 3 = LR)
df = df[df["Study subgroup"].isin([2, 3])]
y = df["Study subgroup"].map({2: 0, 3: 1})  # 0 = HR, 1 = LR

# --- Metadata processing ---
# Compute BMI
# Avoid division by zero
df["BMI"] = df.apply(
    lambda row: row["Weight"] / ((row["Length"]/100) ** 2) if row["Length"] > 0 else None,
    axis=1
)


# Map categorical variables
sex_map = {"male": 0, "female": 1}
smoke_map = {
    "ja": 1,
    "nee, ik ben gestopt met roken": 2,
    "nee, ik heb nooit gerookt": 3
}
alcohol_map = {
    "ik drink geen alcohol": 0,
    "minder dan 1 glas per week": 1,
    "1-5 glazen per week": 2,
    "6-7 glazen per week": 3,
    "8-15 glazen per week": 4,
    "16-30 glazen per week": 5,
    "meer dan 30 glazen per week": 6
}

df["Sex"] = df["Sex"].map(sex_map)
df["Smoking status"] = df["Smoking status"].map(smoke_map)
df["Alcohol"] = df["Alcohol"].map(alcohol_map)

# Select metadata features
meta_features = ["Sex", "Weight", "Length", "BMI", "Smoking status", "Alcohol", "Age"]
X_meta = df[meta_features]

# --- VOC features ---
metabolite_start_col = df.columns.get_loc("Isoprene")
voc_features = df.columns[metabolite_start_col:]
X_voc = df[voc_features]

# Combine VOCs + metadata
X = pd.concat([X_voc, X_meta], axis=1)

# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Random Forest classifier ---
rf = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    class_weight="balanced",
    n_jobs=-1
)
rf.fit(X_train, y_train)

# --- Evaluation ---
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

print("Classification report:\n", classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# --- Cross-validation (optional) ---
cv_scores = cross_val_score(rf, X, y, cv=5, scoring="roc_auc")
print("CV ROC-AUC mean:", np.mean(cv_scores))

# --- Feature importance ---
importances = pd.Series(rf.feature_importances_, index=X.columns)
top_features = importances.sort_values(ascending=False).head(20)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_features.values, y=top_features.index)
plt.title("Top 20 Important Features (Random Forest)")
plt.show()

"""K fold cross validation"""

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score

scoring = {
    "f1_macro": make_scorer(f1_score, average="macro"),
    "precision_macro": make_scorer(precision_score, average="macro"),
    "recall_macro": make_scorer(recall_score, average="macro"),
}

cv_results = cross_validate(rf, X, y, cv=5, scoring=scoring)
print("Average F1 macro:", cv_results['test_f1_macro'].mean())
print("Average precision macro:", cv_results['test_precision_macro'].mean())
print("Average recall macro:", cv_results['test_recall_macro'].mean())

"""Adjust the code for class imbalance using bootstrapping"""

from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# --- Define target ---
df = df[df["Study subgroup"].isin([2, 3])]
y = df["Study subgroup"].map({2: 0, 3: 1})  # 0 = HR (minority), 1 = LR

# --- Metadata processing ---
df["BMI"] = df.apply(
    lambda row: row["Weight"] / ((row["Length"]/100) ** 2) if row["Length"] > 0 else None,
    axis=1
)

sex_map = {"male": 0, "female": 1}
smoke_map = {
    "ja": 1,
    "nee, ik ben gestopt met roken": 2,
    "nee, ik heb nooit gerookt": 3
}
alcohol_map = {
    "ik drink geen alcohol": 0,
    "minder dan 1 glas per week": 1,
    "1-5 glazen per week": 2,
    "6-7 glazen per week": 3,
    "8-15 glazen per week": 4,
    "16-30 glazen per week": 5,
    "meer dan 30 glazen per week": 6
}

df["Sex"] = df["Sex"].map(sex_map)
df["Smoking status"] = df["Smoking status"].map(smoke_map)
df["Alcohol"] = df["Alcohol"].map(alcohol_map)

# --- Select metadata features ---
meta_features = ["Sex", "Weight", "Length", "BMI", "Smoking status", "Alcohol", "Age"]
X_meta = df[meta_features]

# --- VOC features ---
metabolite_start_col = df.columns.get_loc("Isoprene")
voc_features = df.columns[metabolite_start_col:]
X_voc = df[voc_features]

# --- Combine VOCs + metadata ---
X = pd.concat([X_voc, X_meta], axis=1)

# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Upsample minority class (HR = 0) in training set ---
train_data = pd.concat([X_train, y_train], axis=1)
minority = train_data[train_data["Study subgroup"] == 0]
majority = train_data[train_data["Study subgroup"] == 1]

minority_upsampled = resample(
    minority,
    replace=True,  # sample with replacement
    n_samples=len(majority),  # match number of majority samples
    random_state=42
)

train_upsampled = pd.concat([majority, minority_upsampled])
X_train_up = train_upsampled.drop("Study subgroup", axis=1)
y_train_up = train_upsampled["Study subgroup"]

# --- Random Forest classifier ---
rf = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    class_weight="balanced",
    n_jobs=-1
)
rf.fit(X_train_up, y_train_up)

# --- Evaluation ---
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

print("Classification report:\n", classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# --- Cross-validation ---
cv_scores = cross_val_score(rf, X, y, cv=5, scoring="roc_auc")
print("CV ROC-AUC mean:", np.mean(cv_scores))

# --- Feature importance ---
importances = pd.Series(rf.feature_importances_, index=X.columns)
top_features = importances.sort_values(ascending=False).head(20)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_features.values, y=top_features.index)
plt.title("Top 20 Important Features (Random Forest)")
plt.show()

"""K fold cross validation"""

from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
from sklearn.metrics import f1_score, precision_score, recall_score
import numpy as np

# Define the cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

f1_scores, precision_scores, recall_scores = [], [], []

for train_index, test_index in skf.split(X, y):
    # Split fold
    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

    # --- Upsample minority class in training fold ---
    train_data = pd.concat([X_train_fold, y_train_fold], axis=1)
    minority = train_data[train_data["Study subgroup"] == 0]
    majority = train_data[train_data["Study subgroup"] == 1]

    minority_upsampled = resample(
        minority,
        replace=True,
        n_samples=len(majority),
        random_state=42
    )

    train_upsampled = pd.concat([majority, minority_upsampled])
    X_train_up = train_upsampled.drop("Study subgroup", axis=1)
    y_train_up = train_upsampled["Study subgroup"]

    # Train the Random Forest
    rf = RandomForestClassifier(
        n_estimators=500,
        random_state=42,
        class_weight="balanced",
        n_jobs=-1
    )
    rf.fit(X_train_up, y_train_up)

    # Evaluate on test fold
    y_pred = rf.predict(X_test_fold)
    f1_scores.append(f1_score(y_test_fold, y_pred, average="macro"))
    precision_scores.append(precision_score(y_test_fold, y_pred, average="macro"))
    recall_scores.append(recall_score(y_test_fold, y_pred, average="macro"))

# Print average metrics
print("Average F1 macro:", np.mean(f1_scores))
print("Average precision macro:", np.mean(precision_scores))
print("Average recall macro:", np.mean(recall_scores))

"""now we want to train only on top features"""

importances = pd.Series(rf.feature_importances_, index=X.columns)
top_features = importances.sort_values(ascending=False).head(20)  # top 20 features
print(top_features)

X_top = X[top_features.index]  # use only top 20 features

from sklearn.model_selection import train_test_split
X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(
    X_top, y, test_size=0.2, random_state=42, stratify=y
)

rf_top = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    class_weight="balanced",
    n_jobs=-1
)
rf_top.fit(X_train_top, y_train_top)

y_pred_top = rf_top.predict(X_test_top)
y_proba_top = rf_top.predict_proba(X_test_top)[:, 1]

print("Classification report (top features):\n", classification_report(y_test_top, y_pred_top))
print("Confusion matrix:\n", confusion_matrix(y_test_top, y_pred_top))
print("ROC-AUC:", roc_auc_score(y_test_top, y_proba_top))

"""Now with bootstrapping + top 20 features"""

from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# --- Train/test split for top features ---
X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(
    X_top, y, test_size=0.2, random_state=42, stratify=y
)

# --- Upsample minority class (HR = 0) in training set ---
train_top = pd.concat([X_train_top, y_train_top.reset_index(drop=True)], axis=1)
minority = train_top[train_top["Study subgroup"] == 0]
majority = train_top[train_top["Study subgroup"] == 1]

minority_upsampled = resample(
    minority,
    replace=True,
    n_samples=len(majority),  # match majority class
    random_state=42
)

train_upsampled_top = pd.concat([majority, minority_upsampled])
X_train_top_up = train_upsampled_top.drop("Study subgroup", axis=1)
y_train_top_up = train_upsampled_top["Study subgroup"]

# --- Train Random Forest on top features ---
rf_top = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    class_weight=None,
    n_jobs=-1
)
rf_top.fit(X_train_top_up, y_train_top_up)

# --- Evaluate ---
y_pred_top = rf_top.predict(X_test_top)
y_proba_top = rf_top.predict_proba(X_test_top)[:, 1]

print("Classification report (top features):\n", classification_report(y_test_top, y_pred_top))
print("Confusion matrix:\n", confusion_matrix(y_test_top, y_pred_top))
print("ROC-AUC:", roc_auc_score(y_test_top, y_proba_top))

"""# PCA to check for outliers"""

# Import libraries
import pandas as pd
from google.colab import files

# Upload the file
uploaded = files.upload()
filename = next(iter(uploaded))

# Read the Excel file into a DataFrame
df = pd.read_excel(filename, engine='openpyxl')  # Use 'xlrd' if it's .xls

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 1️⃣ Load cleaned & filtered file
df = pd.read_excel("SP_CRC_HR_LR_filtered_last.xlsx", engine='openpyxl')

# 2️⃣ Subset only Study subgroups 2 & 3
df_sub = df[df['Study subgroup'].isin([2, 3])]

# 3️⃣ Select VOC columns
voc_start_col = df_sub.columns.get_loc('Isoprene')
voc_end_col = df_sub.columns.get_loc('2,6-Diphenylphenol')
voc_features = df_sub.columns[voc_start_col:voc_end_col+1]  # inclusive
X = df_sub[voc_features]

# 4️⃣ Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5️⃣ Perform PCA
pca = PCA(n_components=2)
pcs = pca.fit_transform(X_scaled)

# 6️⃣ Create DataFrame for plotting
pc_df = pd.DataFrame(pcs, columns=['PC1', 'PC2'])
pc_df['Group'] = df_sub['Study subgroup'].astype(str)


# 7️⃣ Add tiny jitter to avoid overlapping points
pc_df['PC1_jitter'] = pc_df['PC1'] + np.random.normal(0, 5e-3, size=pc_df.shape[0])
pc_df['PC2_jitter'] = pc_df['PC2'] + np.random.normal(0, 5e-3, size=pc_df.shape[0])

# 8️⃣ Plot PCA with jitter
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='PC1_jitter',
    y='PC2_jitter',
    hue='Group',
    data=pc_df,
    palette='Set1',
    s=50
)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
plt.title('PCA of VOCs for Study Subgroups 2 & 3 (jittered)')
plt.legend(title='Subgroup')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# 1️⃣ Load your cleaned & filtered file
df = pd.read_excel("SP_CRC_HR_LR_filtered_last.xlsx", engine='openpyxl')

# 2️⃣ Keep only Study subgroups 2 & 3
df_sub = df[df['Study subgroup'].isin([2, 3])].copy()
print(f"Number of samples: {df_sub.shape[0]}")

# 3️⃣ Select VOC columns and force numeric
X = df_sub.loc[:, 'Isoprene':'2,6-Diphenylphenol'].apply(pd.to_numeric, errors='coerce')

# 4️⃣ Fill any remaining NaNs with median
X = X.fillna(X.median())

# 5️⃣ Remove zero-variance columns
X_nonzero = X.loc[:, X.var() > 0]
print(f"Number of VOCs after removing zero-variance columns: {X_nonzero.shape[1]}")

# 6️⃣ Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_nonzero)

# 7️⃣ Perform PCA
pca = PCA(n_components=2)
pcs = pca.fit_transform(X_scaled)
print(f"PCA output shape: {pcs.shape}")  # should be (#samples, 2)

# 8️⃣ Create DataFrame for plotting
pc_df = pd.DataFrame(pcs, columns=['PC1', 'PC2'])
pc_df['Group'] = df_sub['Study subgroup'].astype(str)  # categorical for Seaborn

# 9️⃣ Add jitter proportional to PC range
pc1_range = pc_df['PC1'].max() - pc_df['PC1'].min()
pc2_range = pc_df['PC2'].max() - pc_df['PC2'].min()
jitter_amount = 0.01  # 1% of the range
pc_df['PC1_jitter'] = pc_df['PC1'] + np.random.normal(0, pc1_range * jitter_amount, size=pc_df.shape[0])
pc_df['PC2_jitter'] = pc_df['PC2'] + np.random.normal(0, pc2_range * jitter_amount, size=pc_df.shape[0])

# 🔟 Plot PCA with all points visible
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='PC1_jitter',
    y='PC2_jitter',
    hue='Group',
    data=pc_df,
    palette='Set1',
    s=50,
    alpha=0.7
)

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
plt.title('PCA of VOCs for Study Subgroups 2 & 3')
plt.legend(title='Subgroup')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load data and keep only subgroups 2 & 3
df_sub = df[df['Study subgroup'].isin([2, 3])].copy()
X = df_sub.loc[:, 'Isoprene':'2,6-Diphenylphenol'].apply(pd.to_numeric, errors='coerce')
X = X.fillna(X.median())
X_nonzero = X.loc[:, X.var() > 0]

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_nonzero)

# Initial PCA
pca = PCA(n_components=2)
pcs = pca.fit_transform(X_scaled)

# Put PCA results in DataFrame
pc_df = pd.DataFrame(pcs, columns=['PC1','PC2'])
pc_df['index'] = df_sub.index  # keep original indices

# Suppose you already visually know the extreme points on PC1 or PC2
# Here, for example, pick points with PC1 or PC2 far away from the bulk
# (replace this with the actual criteria you saw on the plot)
# We'll find the 11 points with largest absolute PC1 and PC2
pc_df['distance'] = np.sqrt(pc_df['PC1']**2 + pc_df['PC2']**2)
outliers = pc_df.nlargest(11, 'distance')  # exact 11 farthest points

# Extract the original DataFrame rows for these outliers
outlier_rows = df_sub.loc[outliers['index']]

print("Exact 11 outliers:")
print(outlier_rows)

# Remove outliers
df_clean = df_sub.drop(index=outliers['index'])
X_clean = X_nonzero.drop(index=outliers['index'])

# Standardize and redo PCA
X_scaled_clean = scaler.fit_transform(X_clean)
pca_clean = PCA(n_components=2)
pcs_clean = pca_clean.fit_transform(X_scaled_clean)

# Prepare DataFrame for plotting
pc_df_clean = pd.DataFrame(pcs_clean, columns=['PC1','PC2'])
pc_df_clean['Group'] = df_clean['Study subgroup'].astype(str)

# Optional: jitter for visualization
pc1_range = pc_df_clean['PC1'].max() - pc_df_clean['PC1'].min()
pc2_range = pc_df_clean['PC2'].max() - pc_df_clean['PC2'].min()
jitter_amount = 0.01
pc_df_clean['PC1_jitter'] = pc_df_clean['PC1'] + np.random.normal(0, pc1_range*jitter_amount, size=pc_df_clean.shape[0])
pc_df_clean['PC2_jitter'] = pc_df_clean['PC2'] + np.random.normal(0, pc2_range*jitter_amount, size=pc_df_clean.shape[0])

import matplotlib.pyplot as plt
import seaborn as sns

# 🔹 Plot PCA after removing exact outliers
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='PC1_jitter',
    y='PC2_jitter',
    hue='Group',
    data=pc_df_clean,
    palette='Set1',
    s=50,
    alpha=0.7
)

# Add axis labels with variance explained
plt.xlabel(f'PC1 ({pca_clean.explained_variance_ratio_[0]*100:.1f}% variance)')
plt.ylabel(f'PC2 ({pca_clean.explained_variance_ratio_[1]*100:.1f}% variance)')
plt.title('PCA of VOCs for Study Subgroups 2 & 3 (outliers removed)')
plt.legend(title='Subgroup')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# 1️⃣ Load data
df = pd.read_excel("SP_CRC_HR_LR_filtered_last.xlsx", engine='openpyxl')

# 2️⃣ Keep only subgroups 2 & 3
df_sub = df[df['Study subgroup'].isin([2, 3])].copy()

# 3️⃣ Select VOC columns and force numeric
X = df_sub.loc[:, 'Isoprene':'2,6-Diphenylphenol'].apply(pd.to_numeric, errors='coerce')
X = X.fillna(X.median())
X_nonzero = X.loc[:, X.var() > 0]

# 4️⃣ Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_nonzero)

# 5️⃣ Initial PCA to identify outliers
pca_initial = PCA(n_components=2)
pcs_initial = pca_initial.fit_transform(X_scaled)
pc_df_initial = pd.DataFrame(pcs_initial, columns=['PC1','PC2'])
pc_df_initial['index'] = df_sub.index
pc_df_initial['distance'] = np.sqrt(pc_df_initial['PC1']**2 + pc_df_initial['PC2']**2)

# 6️⃣ Identify exact 11 outliers (largest distances)
outliers = pc_df_initial.nlargest(11, 'distance')

# 7️⃣ Remove these outliers
non_outliers_mask = ~pc_df_initial['index'].isin(outliers['index'])
df_clean = df_sub[non_outliers_mask]
X_clean = X_nonzero[non_outliers_mask]

# 8️⃣ Standardize again and redo PCA on non-outliers
X_scaled_clean = scaler.fit_transform(X_clean)
pca_clean = PCA(n_components=2)
pcs_clean = pca_clean.fit_transform(X_scaled_clean)

# 9️⃣ Prepare DataFrame for plotting
pc_df_clean = pd.DataFrame(pcs_clean, columns=['PC1','PC2'])
pc_df_clean['Batch'] = df_clean['Seq'].astype(str)
pc_df_clean['Group'] = df_clean['Study subgroup'].astype(str)

# 🔟 Add small jitter for visibility
pc1_range = pc_df_clean['PC1'].max() - pc_df_clean['PC1'].min()
pc2_range = pc_df_clean['PC2'].max() - pc_df_clean['PC2'].min()
jitter_amount = 0.01
pc_df_clean['PC1_jitter'] = pc_df_clean['PC1'] + np.random.normal(0, pc1_range*jitter_amount, size=pc_df_clean.shape[0])
pc_df_clean['PC2_jitter'] = pc_df_clean['PC2'] + np.random.normal(0, pc2_range*jitter_amount, size=pc_df_clean.shape[0])

# 1️⃣1️⃣ Plot PCA colored by batch
plt.figure(figsize=(10,7))
sns.scatterplot(
    x='PC1_jitter',
    y='PC2_jitter',
    hue='Batch',
    palette='tab20',  # enough colors for 12 batches
    data=pc_df_clean,
    s=60,
    alpha=0.8
)
plt.xlabel(f'PC1 ({pca_clean.explained_variance_ratio_[0]*100:.1f}% variance)')
plt.ylabel(f'PC2 ({pca_clean.explained_variance_ratio_[1]*100:.1f}% variance)')
plt.title('PCA of VOCs colored by Batch (Seq) — Subgroups 2 & 3, outliers removed')
plt.legend(title='Batch', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Test PC1 vs Batch
pc_df_clean['PC1'] = pcs_clean[:,0]
pc_df_clean['PC2'] = pcs_clean[:,1]

model_pc1 = ols('PC1 ~ C(Batch)', data=pc_df_clean).fit()
print(sm.stats.anova_lm(model_pc1))

model_pc2 = ols('PC2 ~ C(Batch)', data=pc_df_clean).fit()
print(sm.stats.anova_lm(model_pc2))

from statsmodels.multivariate.manova import MANOVA
import numpy as np

# Decide how many PCs to keep (e.g., enough to explain 80% variance)
pca_full = PCA()
X_scaled_clean = scaler.fit_transform(X_clean)
pcs_full = pca_full.fit_transform(X_scaled_clean)

explained_cumsum = np.cumsum(pca_full.explained_variance_ratio_)
n_components = np.argmax(explained_cumsum >= 0.8) + 1  # first PC index to reach 80%

print(f"Number of PCs to explain 80% variance: {n_components}")

# Prepare DataFrame with these PCs
pc_cols = [f'PC{i+1}' for i in range(n_components)]
pc_df_full = pd.DataFrame(pcs_full[:, :n_components], columns=pc_cols)
pc_df_full['Batch'] = df_clean['Seq'].astype(str)

# Run MANOVA
formula = ' + '.join(pc_cols) + ' ~ C(Batch)'
manova = MANOVA.from_formula(formula, data=pc_df_full)
print(manova.mv_test())

import numpy as np
from itertools import combinations
import matplotlib.pyplot as plt
import seaborn as sns

def bhattacharyya_distance(mu1, cov1, mu2, cov2):
    """
    Compute Bhattacharyya distance between two multivariate Gaussians.
    """
    cov_avg = (cov1 + cov2) / 2
    diff = mu2 - mu1
    term1 = 0.125 * diff.T @ np.linalg.inv(cov_avg) @ diff
    term2 = 0.5 * np.log(np.linalg.det(cov_avg) / np.sqrt(np.linalg.det(cov1) * np.linalg.det(cov2)))
    return term1 + term2

# Use first N PCs (e.g., all 26 you kept)

pc_cols = ['PC1', 'PC2']  # only the PCs available


# Get unique Batch values
batches = pc_df_clean['Batch'].unique()

# Compute pairwise Bhattacharyya distances
distances = {}
for batch1, batch2 in combinations(batches, 2):
    data1 = pc_df_clean[pc_df_clean['Batch'] == batch1][pc_cols].values
    data2 = pc_df_clean[pc_df_clean['Batch'] == batch2][pc_cols].values

    mu1, cov1 = data1.mean(axis=0), np.cov(data1, rowvar=False)
    mu2, cov2 = data2.mean(axis=0), np.cov(data2, rowvar=False)

    dist = bhattacharyya_distance(mu1, cov1, mu2, cov2)
    distances[(batch1, batch2)] = dist

# Convert distances to a matrix for heatmap
batch_list = list(batches)
dist_matrix = np.zeros((len(batch_list), len(batch_list)))
for i, batch1 in enumerate(batch_list):
    for j, batch2 in enumerate(batch_list):
        if i == j:
            dist_matrix[i, j] = 0
        elif i < j:
            dist_matrix[i, j] = distances[(batch1, batch2)]
        else:
            dist_matrix[i, j] = dist_matrix[j, i]

# Plot heatmap
plt.figure(figsize=(10,8))
sns.heatmap(dist_matrix, xticklabels=batch_list, yticklabels=batch_list, cmap='viridis', annot=True, fmt=".2f")
plt.title("Bhattacharyya distances between Batch groups")
plt.show()

import numpy as np
from itertools import combinations
import pandas as pd

def bhattacharyya_distance(mu1, cov1, mu2, cov2):
    cov_avg = (cov1 + cov2) / 2
    diff = mu2 - mu1
    term1 = 0.125 * diff.T @ np.linalg.inv(cov_avg) @ diff
    term2 = 0.5 * np.log(np.linalg.det(cov_avg) / np.sqrt(np.linalg.det(cov1) * np.linalg.det(cov2)))
    return term1 + term2

# Use the PCs available
pc_cols = ['PC1', 'PC2']

# Get unique batches
batches = pc_df_clean['Batch'].unique()

# Compute pairwise distances
results = []
for batch1, batch2 in combinations(batches, 2):
    data1 = pc_df_clean[pc_df_clean['Batch'] == batch1][pc_cols].values
    data2 = pc_df_clean[pc_df_clean['Batch'] == batch2][pc_cols].values

    mu1, cov1 = data1.mean(axis=0), np.cov(data1, rowvar=False)
    mu2, cov2 = data2.mean(axis=0), np.cov(data2, rowvar=False)

    dist = bhattacharyya_distance(mu1, cov1, mu2, cov2)
    results.append([batch1, batch2, dist])

# Convert to DataFrame for easy viewing
dist_df = pd.DataFrame(results, columns=['Batch1', 'Batch2', 'Bhattacharyya_distance'])
print(dist_df)

print("Average Bhattacharyya distance between batches:", dist_df['Bhattacharyya_distance'].mean())
print("Maximum Bhattacharyya distance between batches:", dist_df['Bhattacharyya_distance'].max())

pip install neuroCombat

import pandas as pd
from neuroCombat import neuroCombat

# X_clean: VOCs after removing outliers (samples x features)
# df_clean: metadata for these samples

# Step 1: Create covariates DataFrame including batch
covars = pd.DataFrame({
    'Seq': df_clean['Seq'],                 # batch column must be here
    'Study_subgroup': df_clean['Study subgroup']  # biological covariate
}, index=df_clean.index)

# Step 2: Transpose data for neuroCombat (features x samples)
data = X_clean.T

# Step 3: Run ComBat batch correction
combat_result = neuroCombat(
    dat=data,
    covars=covars,
    batch_col='Seq',                     # must match column name in covars
    categorical_cols=['Study_subgroup'], # biological variable to preserve
    continuous_cols=[]                   # none in this case
)

# Step 4: Extract corrected data and transpose back
X_combat = pd.DataFrame(combat_result['data'].T,
                        index=X_clean.index,
                        columns=X_clean.columns)

print("Batch-corrected VOC data shape:", X_combat.shape)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_combat)

# PCA
pca = PCA(n_components=2)
pcs = pca.fit_transform(X_scaled)

# Prepare DataFrame for plotting
pc_df = pd.DataFrame(pcs, columns=['PC1','PC2'])
pc_df['Batch'] = df_clean['Seq'].astype(str)
pc_df['Group'] = df_clean['Study subgroup'].astype(str)

# Optional: small jitter for visibility
pc1_range = pc_df['PC1'].max() - pc_df['PC1'].min()
pc2_range = pc_df['PC2'].max() - pc_df['PC2'].min()
jitter_amount = 0.01
pc_df['PC1_jitter'] = pc_df['PC1'] + np.random.normal(0, pc1_range*jitter_amount, size=pc_df.shape[0])
pc_df['PC2_jitter'] = pc_df['PC2'] + np.random.normal(0, pc2_range*jitter_amount, size=pc_df.shape[0])

# Plot PCA colored by batch
plt.figure(figsize=(10,7))
sns.scatterplot(
    x='PC1_jitter',
    y='PC2_jitter',
    hue='Batch',
    data=pc_df,
    palette='tab20',
    s=60,
    alpha=0.8
)
plt.title('PCA after ComBat batch correction')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
plt.legend(title='Batch', bbox_to_anchor=(1.05,1), loc='upper left')
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from itertools import combinations
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# --- Step 1: Standardize and reduce to 2 PCs ---
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_combat)  # batch-corrected data

pca = PCA(n_components=2)
pcs = pca.fit_transform(X_scaled)

pc_df = pd.DataFrame(pcs, columns=['PC1','PC2'])
pc_df['Batch'] = df_clean['Seq'].astype(str)  # batch info

# --- Step 2: Define regularized Bhattacharyya distance ---
def bhattacharyya_distance(mu1, cov1, mu2, cov2, epsilon=1e-6):
    # Regularize covariance matrices
    cov1 += np.eye(cov1.shape[0]) * epsilon
    cov2 += np.eye(cov2.shape[0]) * epsilon
    cov_avg = (cov1 + cov2) / 2
    diff = mu2 - mu1
    term1 = 0.125 * diff.T @ np.linalg.inv(cov_avg) @ diff
    term2 = 0.5 * np.log(np.linalg.det(cov_avg) / np.sqrt(np.linalg.det(cov1) * np.linalg.det(cov2)))
    return term1 + term2

# --- Step 3: Compute pairwise distances ---
batches = pc_df['Batch'].unique()
results = []

for batch1, batch2 in combinations(batches, 2):
    data1 = pc_df[pc_df['Batch'] == batch1][['PC1','PC2']].values
    data2 = pc_df[pc_df['Batch'] == batch2][['PC1','PC2']].values

    mu1, cov1 = data1.mean(axis=0), np.cov(data1, rowvar=False)
    mu2, cov2 = data2.mean(axis=0), np.cov(data2, rowvar=False)

    dist = bhattacharyya_distance(mu1, cov1, mu2, cov2)
    results.append([batch1, batch2, dist])

dist_df = pd.DataFrame(results, columns=['Batch1','Batch2','Bhattacharyya_distance'])

# --- Step 4: Summary statistics ---
print("Bhattacharyya distances after ComBat:")
print(dist_df)
print("\nAverage distance:", dist_df['Bhattacharyya_distance'].mean())
print("Maximum distance:", dist_df['Bhattacharyya_distance'].max())

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Use the PCA you just did on X_combat
pc_df['Batch'] = df_clean['Seq'].astype(str)
pc_df['Group'] = df_clean['Study subgroup'].astype(str)

# ANOVA: PC1 ~ Batch
model_pc1 = ols('PC1 ~ C(Batch)', data=pc_df).fit()
anova_pc1 = sm.stats.anova_lm(model_pc1)
print("ANOVA for PC1 ~ Batch after ComBat")
print(anova_pc1)

# ANOVA: PC2 ~ Batch
model_pc2 = ols('PC2 ~ C(Batch)', data=pc_df).fit()
anova_pc2 = sm.stats.anova_lm(model_pc2)
print("ANOVA for PC2 ~ Batch after ComBat")
print(anova_pc2)

import pandas as pd
from google.colab import files

# Combine batch-corrected VOCs with metadata
df_final = pd.concat([df_clean.reset_index(drop=True), X_combat.reset_index(drop=True)], axis=1)

# Save to Excel
file_name = "SP_CRC_HR_LR_cleaned_batch_corrected.xlsx"
df_final.to_excel(file_name, index=False)

# Trigger download
files.download(file_name)

# Compare pre- and post-batch correction for Isoprene
df_pre = df_clean['Isoprene']
df_post = X_combat['Isoprene']

comparison = pd.DataFrame({'Before': df_pre, 'After': df_post})
print(comparison.head(10))  # first 10 rows
print("Any differences?", (df_pre != df_post).any())

import pandas as pd
from google.colab import files

# Upload the new Excel file
uploaded = files.upload()
filename = next(iter(uploaded))

# Read it into a new DataFrame
df_cleaned = pd.read_excel(filename, engine='openpyxl')
print(f"Shape of new dataset: {df_cleaned.shape}")

from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# --- Define target ---
df_rf = df_clean.copy()  # df_clean = outliers removed
df_rf = df_rf[df_rf["Study subgroup"].isin([2, 3])]
y = df_rf["Study subgroup"].map({2: 0, 3: 1})  # 0 = HR, 1 = LR

# --- Metadata processing ---
df_rf["BMI"] = df_rf.apply(
    lambda row: row["Weight"] / ((row["Length"]/100) ** 2) if row["Length"] > 0 else None,
    axis=1
)

sex_map = {"male": 0, "female": 1}
smoke_map = {
    "ja": 1,
    "nee, ik ben gestopt met roken": 2,
    "nee, ik heb nooit gerookt": 3
}
alcohol_map = {
    "ik drink geen alcohol": 0,
    "minder dan 1 glas per week": 1,
    "1-5 glazen per week": 2,
    "6-7 glazen per week": 3,
    "8-15 glazen per week": 4,
    "16-30 glazen per week": 5,
    "meer dan 30 glazen per week": 6
}

df_rf["Sex"] = df_rf["Sex"].map(sex_map)
df_rf["Smoking status"] = df_rf["Smoking status"].map(smoke_map)
df_rf["Alcohol"] = df_rf["Alcohol"].map(alcohol_map)

# --- Metadata features ---
meta_features = ["Sex", "Weight", "Length", "BMI", "Smoking status", "Alcohol", "Age"]
X_meta = df_rf[meta_features]

# --- VOC features (batch-corrected) ---
# X_combat must have same index as df_rf
X_voc = X_combat.loc[df_rf.index, :]  # select the correct rows

# --- Combine VOCs + metadata ---
X = pd.concat([X_voc, X_meta], axis=1)

# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Upsample minority class in training set ---
train_data = pd.concat([X_train, y_train], axis=1)
minority = train_data[train_data["Study subgroup"] == 0]
majority = train_data[train_data["Study subgroup"] == 1]

minority_upsampled = resample(
    minority,
    replace=True,
    n_samples=len(majority),
    random_state=42
)

train_upsampled = pd.concat([majority, minority_upsampled])
X_train_up = train_upsampled.drop("Study subgroup", axis=1)
y_train_up = train_upsampled["Study subgroup"]

# --- Random Forest classifier ---
rf = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    class_weight="balanced",
    n_jobs=-1
)
rf.fit(X_train_up, y_train_up)

# --- Evaluation ---
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

print("Classification report:\n", classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# --- Cross-validation ---
cv_scores = cross_val_score(rf, X, y, cv=5, scoring="roc_auc")
print("CV ROC-AUC mean:", np.mean(cv_scores))

# --- Feature importance ---
importances = pd.Series(rf.feature_importances_, index=X.columns)
top_features = importances.sort_values(ascending=False).head(30)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_features.values, y=top_features.index)
plt.title("Top 20 Important Features (Random Forest)")
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 1️⃣ Load batch-corrected VOCs and metadata
# X_combat = batch-corrected VOCs (samples x features)
# df_clean = metadata for these samples

# 2️⃣ Keep only subgroups 2 & 3
df_sub = df_clean[df_clean['Study subgroup'].isin([2, 3])].copy()
X_voc = X_combat.loc[df_sub.index]  # align with metadata

# 3️⃣ Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_voc)

# 4️⃣ PCA
pca = PCA(n_components=2)
pcs = pca.fit_transform(X_scaled)

# 5️⃣ Create DataFrame for plotting
pc_df = pd.DataFrame(pcs, columns=['PC1', 'PC2'])
pc_df['Group'] = df_sub['Study subgroup'].astype(str)

# 6️⃣ Optional: add small jitter to see overlapping points
pc1_range = pc_df['PC1'].max() - pc_df['PC1'].min()
pc2_range = pc_df['PC2'].max() - pc_df['PC2'].min()
jitter_amount = 0.01
pc_df['PC1_jitter'] = pc_df['PC1'] + np.random.normal(0, pc1_range*jitter_amount, size=pc_df.shape[0])
pc_df['PC2_jitter'] = pc_df['PC2'] + np.random.normal(0, pc2_range*jitter_amount, size=pc_df.shape[0])

# 7️⃣ Plot
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='PC1_jitter',
    y='PC2_jitter',
    hue='Group',
    data=pc_df,
    palette='Set1',
    s=60,
    alpha=0.8
)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
plt.title('PCA of Batch-Corrected VOCs (Subgroups 2 & 3)')
plt.legend(title='Subgroup')
plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define parameter grid
param_grid = {
    'n_estimators': [200, 500, 800],
    'max_depth': [None, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(random_state=42, oob_score=True, class_weight='balanced')

grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)

grid_search.fit(X_train_up, y_train_up)

print("Best parameters:", grid_search.best_params_)
print("Best CV ROC-AUC:", grid_search.best_score_)
print("OOB score:", grid_search.best_estimator_.oob_score_)

from sklearn.metrics import roc_curve, auc

# Train RF with OOB on full data
rf_optimized = grid_search.best_estimator_
rf_optimized.fit(X_train_up, y_train_up)

# OOB predictions
oob_proba = rf_optimized.oob_decision_function_[:, 1]  # probability for class 1
fpr_oob, tpr_oob, _ = roc_curve(y_train_up, oob_proba)
roc_auc_oob = auc(fpr_oob, tpr_oob)

# CV predictions (using cross_val_predict)
from sklearn.model_selection import cross_val_predict
cv_proba = cross_val_predict(rf_optimized, X, y, cv=5, method='predict_proba')[:,1]
fpr_cv, tpr_cv, _ = roc_curve(y, cv_proba)
roc_auc_cv = auc(fpr_cv, tpr_cv)

# Plot both ROC curves
import matplotlib.pyplot as plt
plt.figure(figsize=(8,6))
plt.plot(fpr_oob, tpr_oob, label=f'OOB ROC (AUC={roc_auc_oob:.3f})')
plt.plot(fpr_cv, tpr_cv, label=f'CV ROC (AUC={roc_auc_cv:.3f})')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('OOB vs CV ROC Curves')
plt.legend()
plt.show()

pd.crosstab(df_clean['Seq'], df_clean['Study subgroup'])

from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# --- 1️⃣ Define target and subset ---
df_rf = df_clean.copy()  # already removed outliers
df_rf = df_rf[df_rf["Study subgroup"].isin([2, 3])]
y = df_rf["Study subgroup"].map({2: 0, 3: 1})  # 0=HR, 1=LR

# --- 2️⃣ Metadata processing ---
df_rf["BMI"] = df_rf.apply(lambda row: row["Weight"] / ((row["Length"]/100) ** 2) if row["Length"] > 0 else None, axis=1)

# Select metadata features
meta_features = ["Sex", "Weight", "Length", "BMI", "Smoking status", "Alcohol", "Age"]
X_meta_raw = df_rf[meta_features]

# One-hot encode categorical metadata
X_meta = pd.get_dummies(
    X_meta_raw,
    columns=["Sex", "Smoking status", "Alcohol"],
    drop_first=False  # keep all categories
)

# --- 3️⃣ VOC features (batch-corrected) ---
X_voc = X_combat.loc[df_rf.index, :]  # make sure rows match metadata

# --- 4️⃣ Combine VOCs + metadata ---
X = pd.concat([X_voc, X_meta], axis=1)

# --- 5️⃣ Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- 6️⃣ Upsample minority class in training set ---
train_data = pd.concat([X_train, y_train], axis=1)
minority = train_data[train_data["Study subgroup"] == 0]
majority = train_data[train_data["Study subgroup"] == 1]

minority_upsampled = resample(
    minority,
    replace=True,
    n_samples=len(majority),
    random_state=42
)

train_upsampled = pd.concat([majority, minority_upsampled])
X_train_up = train_upsampled.drop("Study subgroup", axis=1)
y_train_up = train_upsampled["Study subgroup"]

# --- 7️⃣ Random Forest classifier ---
rf = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    class_weight="balanced",
    n_jobs=-1
)
rf.fit(X_train_up, y_train_up)

# --- 8️⃣ Evaluation ---
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

print("Classification report:\n", classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# --- 9️⃣ Cross-validation ---
cv_scores = cross_val_score(rf, X, y, cv=5, scoring="roc_auc")
print("CV ROC-AUC mean:", np.mean(cv_scores))

# --- 🔟 Feature importance stability ---
top_features_list = []

for i in range(10):  # run RF 10 times with different seeds
    rf_tmp = RandomForestClassifier(
        n_estimators=500,
        random_state=i,
        class_weight="balanced",
        n_jobs=-1
    )
    rf_tmp.fit(X_train_up, y_train_up)
    importances = pd.Series(rf_tmp.feature_importances_, index=X.columns)
    top_features_list.append(importances.sort_values(ascending=False).head(20))

# Count how often each feature appears in top 20
all_top_features = [feat for run in top_features_list for feat in run.index]
feature_counts = Counter(all_top_features)
print("Most consistently important features (top 20 across 10 runs):")
print(feature_counts.most_common(20))

# --- 1️⃣1️⃣ Plot top features from last RF run ---
importances = pd.Series(rf.feature_importances_, index=X.columns)
top_features = importances.sort_values(ascending=False).head(20)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_features.values, y=top_features.index)
plt.title("Top 20 Important Features (Random Forest)")
plt.tight_layout()
plt.show()
